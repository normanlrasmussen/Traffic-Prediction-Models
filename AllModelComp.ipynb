{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Compares all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, log_loss, brier_score_loss, roc_auc_score, roc_curve, \n",
    "                             precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a dictionary where we will keep all of the data\n",
    "model_performances = {}\n",
    "\n",
    "#Define the df\n",
    "df = pd.read_csv(\"data\\\\Traffic.csv\")\n",
    "\n",
    "#Get the inputs and target\n",
    "X = df.drop(columns=\"Traffic Situation\")\n",
    "y = df[\"Traffic Situation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will hadel the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the data for the NN\n",
    "\"\"\"\n",
    "X[\"day_of_the_month_sin\"] = np.sin(2 * np.pi * X[\"Date\"] / 31)\n",
    "X[\"day_of_the_month_cos\"] = np.cos(2 * np.pi * X[\"Date\"] / 31)\n",
    "X = X.drop(columns=\"Date\")\n",
    "\n",
    "day_mapping = {\n",
    "    'Monday': 0,\n",
    "    'Tuesday': 1,\n",
    "    'Wednesday': 2,\n",
    "    'Thursday': 3,\n",
    "    'Friday': 4,\n",
    "    'Saturday': 5,\n",
    "    'Sunday': 6\n",
    "}\n",
    "\n",
    "X['Day of the week'] = X['Day of the week'].map(day_mapping)\n",
    "\n",
    "X[\"hour\"] = pd.to_datetime(X[\"Time\"], format='%I:%M:%S %p').dt.hour  \n",
    "X[\"minute\"] = pd.to_datetime(X[\"Time\"], format='%I:%M:%S %p').dt.minute  \n",
    "X[\"Time\"] = X[\"hour\"] * 60 + X[\"minute\"]\n",
    "\n",
    "X[\"Time_sin\"] = np.sin(2 * np.pi * X[\"Time\"] / 1425)\n",
    "X[\"Time_cos\"] = np.cos(2 * np.pi * X[\"Time\"] / 1425)\n",
    "X = X.drop(columns=['hour', 'minute', 'Time'])\n",
    "\n",
    "target_mapping = {\n",
    "    'heavy': 0,\n",
    "    'high' : 1,\n",
    "    'normal' : 2,\n",
    "    'low' : 3\n",
    "}\n",
    "\n",
    "y = y.map(target_mapping)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "columns_to_normalize = ['Day of the week', 'CarCount', 'BikeCount', 'BusCount', 'TruckCount', 'Total', \n",
    "                        'day_of_the_month_sin', 'day_of_the_month_cos', 'Time_sin', 'Time_cos']\n",
    "\n",
    "X[columns_to_normalize] = scaler.fit_transform(X[columns_to_normalize])\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, train_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrafficNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(10,20)\n",
    "        self.layer2 = nn.Linear(20,10)\n",
    "        self.layer3 = nn.Linear(10,4)\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrafficNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "#TODO, put in the optimal nn parameters once you find them\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train) \n",
    "    loss = criterion(outputs, y_train) \n",
    "    loss.backward() \n",
    "    optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)  # Forward pass\n",
    "    y_pred = torch.argmax(outputs, dim=1)\n",
    "\n",
    "y_test = y_test.numpy()\n",
    "y_pred = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will handel the Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell gets the data ready to get passed into a pipeline\n",
    "\"\"\"\n",
    "day_mapping = {\n",
    "    'Monday': 0,\n",
    "    'Tuesday': 1,\n",
    "    'Wednesday': 2,\n",
    "    'Thursday': 3,\n",
    "    'Friday': 4,\n",
    "    'Saturday': 5,\n",
    "    'Sunday': 6\n",
    "}\n",
    "\n",
    "X['Day of the week'] = X['Day of the week'].map(day_mapping)\n",
    "\n",
    "X[\"hour\"] = pd.to_datetime(X[\"Time\"], format='%I:%M:%S %p').dt.hour  \n",
    "X[\"minute\"] = pd.to_datetime(X[\"Time\"], format='%I:%M:%S %p').dt.minute  \n",
    "X[\"Time\"] = X[\"hour\"] * 60 + X[\"minute\"]\n",
    "X = X.drop(columns=['hour', 'minute'])\n",
    "\n",
    "target_mapping = {\n",
    "    'heavy': 0,\n",
    "    'high' : 1,\n",
    "    'normal' : 2,\n",
    "    'low' : 3\n",
    "}\n",
    "\n",
    "y = y.map(target_mapping)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Date', 'Time', 'CarCount', 'BikeCount', 'BusCount', 'TruckCount', 'Total']\n",
    "categorical_features = ['Day of the week']\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"scalar\", StandardScaler()) #We're gonna normalize the numeric data \n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")) #Shouldn't have to worry about unknowns \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numerical_pipeline, numeric_features),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"svc\", SVC(\n",
    "        C = 100, \n",
    "        gamma = 'auto', \n",
    "        kernel = 'rbf'\n",
    "    ))\n",
    "])\n",
    "\n",
    "GBC_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('gbc', GradientBoostingClassifier(\n",
    "        learning_rate = 0.1, \n",
    "        max_depth = 3, \n",
    "        max_features = 'log2', \n",
    "        n_estimators = 500, \n",
    "        subsample = 1.0\n",
    "    ))\n",
    "])\n",
    "\n",
    "LogReg_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('logreg', LogisticRegression(\n",
    "        C = 100, \n",
    "        solver = 'lbfgs'\n",
    "    ))\n",
    "])\n",
    "\n",
    "RFC_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(\n",
    "        max_depth = 10, \n",
    "        max_features = 'sqrt', \n",
    "        min_samples_leaf = 2, \n",
    "        min_samples_split = 2, \n",
    "        n_estimators = 100,\n",
    "        random_state = 42\n",
    "    ))\n",
    "])\n",
    "\n",
    "XGB_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", XGBClassifier(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=3, \n",
    "        eval_metric=\"logloss\", \n",
    "        random_state=42,\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"SVC\", \"GBC\", \"Log\", \"RFC\", \"XGB\"]\n",
    "models = [SVC_pipeline, GBC_pipeline, LogReg_pipeline, RFC_pipeline, XGB_pipeline]\n",
    "\n",
    "for name, model in zip(model_names, models):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #Figure out which ones you want\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will hand Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the visualization of all the data will be visualized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
